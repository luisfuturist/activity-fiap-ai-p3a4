{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIAP - Atividade F3A4 - Grupo 61\n",
    "\n",
    "> Esse projeto é parte do curso de **Inteligência Artificial** da [FIAP](https://github.com/fiap) - Online 2024. Este notebook é a atividade \"**Fase 3** Atividade Cap. 14 - A primeira técnica de aprendizado de máquina.\"\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. Desenvolver análise exploratória dos dados\n",
    "2. Desenvolver análise descritiva dos dados\n",
    "3. Responder ao seguinte questionamento: `Encontrar o “perfil ideal” de solo/clima para as plantações, além de discorrer sobre como os três produtos distintos (à escolha do grupo) se comparam com esse perfil ideal. Por exemplo, preferem maior umidade e mais precipitação? Preferem mais calor e menos fósforo? Para esta parte se apoie em análises estatísticas e/ou visuais;`\n",
    "4. Desenvolver modelos preditivos de classificação para os dados\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Bruno Conterato (RM561048)\n",
    "- Luis Emidio (RM559976)\n",
    "- Willian Pinheiro Marques (RM560402)\n",
    "- Roberto Besser (RM559400)\n",
    "- Ludimila Vitorino (RM559697)\n",
    "\n",
    "## Professores\n",
    "\n",
    "- Tutor: <a href=\"https://www.linkedin.com/in/lucas-gomes-moreira-15a8452a/\">Lucas Gomes Moreira</a>\n",
    "- Coordenador: <a href=\"https://www.linkedin.com/in/profandregodoi/\">André Godoi</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definição do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from scipy.stats import zscore\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dark theme for plots\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Compreensão dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Visualização em tabela dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantidade de linhas: \", df.shape[0])\n",
    "print(\"Quantidade de colunas: \", df.shape[1])\n",
    "\n",
    "# Display 5 random samples from the dataset\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Entendendo os dados\n",
    "\n",
    "**Variáveis numéricas discretas**\n",
    "- N: concentração de potássio (unidade não informada)\n",
    "- P: concentração de fósforo (unidade não informada)\n",
    "- K: concentração de potássio (unidade não informada)\n",
    "\n",
    "**Variáveis numéricas contínuas**\n",
    "- temperature: temperatura (ºC)\n",
    "- humidity: umidade (%)\n",
    "- ph: H do solo\n",
    "- rainfall: quantidade de precipitação (mm)\n",
    "\n",
    "**Variaveis categóricas**\n",
    "- label: cultura agrícola\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Visualizando as classes (Culturas agrícolas) \n",
    "\n",
    "- Culturas agrícolas disponíveis:\n",
    "  - rice (arroz)\n",
    "  - maize (milho)\n",
    "  - chickpea (grão-de-bico)\n",
    "  - kidneybeans (feijão)\n",
    "  - pigeonpeas (ervilhas)\n",
    "  - mothbeans (feijão-moth)\n",
    "  - mungbean (feijão-mungo)\n",
    "  - blackgram (feijão-preto)\n",
    "  - lentil (lentilha)\n",
    "  - pomegranate (romã)\n",
    "  - banana (banana)\n",
    "  - mango (manga)\n",
    "  - grapes (uvas)\n",
    "  - watermelon (melancia)\n",
    "  - muskmelon (melão)\n",
    "  - apple (maçã)\n",
    "  - orange (laranja)\n",
    "  - papaya (papaia)\n",
    "  - coconut (coco)\n",
    "  - cotton (algodão)\n",
    "  - jute (juta)\n",
    "  - coffee (café)\n",
    "- Total de culturas agrícolas: 22\n",
    "- Cada cultura agrícola possui 100 registros\n",
    "- Total de registros: 2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem dos valores para cada classe\n",
    "classes = df['label'].value_counts()\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Análise de Inconsistências\n",
    "\n",
    "**Investigação de outliers**\n",
    "\n",
    "- Cada cultura agrícola é cultivada em diferentes condições de solo, temperatura, umidade, pH e precipitação\n",
    "- Portanto, não faz sentido considerar outliers nos dados entre culturas diferentes\n",
    "- A análise de outliers deve, portanto, ser feita para cada cultura agrícola individualmente\n",
    "\n",
    "**Método de detecção de outliers**\n",
    "\n",
    "Vamos utilizar o método do z_score para detectar outliers.\n",
    "\n",
    "Cálculo do z_score:\n",
    "\n",
    "`z = (X - μ) / σ`\n",
    "- X é o valor da amostra\n",
    "- μ é a média da população\n",
    "- σ é o desvio padrão da população\n",
    "\n",
    "O z_score é uma medida de quantos desvios padrão um ponto de dados está longe da média. Se o z_score de um ponto de dados for maior que 2, consideramos o ponto como um outlier.\n",
    "\n",
    "Consideremos a população como sendo dados de cada coluna para cada cultura agrícola.\n",
    "\n",
    "**Outlivers encontrados**\n",
    "- Apenas 2 pontos outliers encontrados: para a cultura agrícola 'apple' (maçã) na variável 'ph'\n",
    "  \n",
    "**Conclusão**\n",
    "- Os dados não contém quantidade significativa de outliers, sendo assim não foi necessário a correção de inconsistências.\n",
    "- Remoção de duplicatas são não-aplicáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df.select_dtypes(include=[float, int]).columns\n",
    "for crop in df['label'].unique():\n",
    "    print(f\"Outliers para a classe {crop}\")\n",
    "    z_scores = df[df['label'] == crop][feature_columns].apply(zscore)\n",
    "    outliers = (z_scores.abs() > 2).sum()\n",
    "    \n",
    "    if len(outliers[outliers > 0]) == 0:\n",
    "        print(\"Nenhum outlier encontrado\")\n",
    "        print(\"\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(outliers[outliers > 0])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter z-scores for 'apple' crop\n",
    "apple_z_scores = df[df['label'] == 'apple'][feature_columns].apply(zscore)\n",
    "\n",
    "# Find outliers in 'ph' variable\n",
    "apple_ph_outliers = apple_z_scores[apple_z_scores['ph'].abs() > 2]\n",
    "# Print the top 10 absolute z-scores for 'apple' crop\n",
    "top_10_z_scores_indices = apple_z_scores['ph'].abs().sort_values(ascending=False).head(10).index\n",
    "\n",
    "print(\"Top 10 absolute z-scores for 'apple' crop with their 'ph' values:\")\n",
    "for idx in top_10_z_scores_indices:\n",
    "    print(f\"Index: {idx}, z_score: {apple_z_scores.loc[idx, 'ph']}, ph value: {df.loc[idx, 'ph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5. Buscando correlações entre as variáveis\n",
    "\n",
    "- Em termos gerais, as correlações entre as variáveis são baixas (valores absolutos menores que 23.14%)\n",
    "- A excessão: a correlação entre as variáveis N e K é de 73.62%, considerada elevada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation table\n",
    "corr_df = df.corr(numeric_only=True)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6. Estatísticas descritivas básicas das variáveis numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7. Análise de valores nulos\n",
    "\n",
    "Não há valores nulos nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar valores nulos por coluna\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análise descritiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Gráficos boxplot de cada variável por cultura agrícola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Boxplot Nitrogênio (N) por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de concentração de nitrogênio (N) ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='N', palette='Set2', legend=False)\n",
    "plt.title('Nitrogen Levels by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Nitrogen (N) Concentration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Boxplot Fósforo (P) por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de concentração de fósforo (P) ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='P', palette='Set2')\n",
    "plt.title('Phosphorus Levels by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Phosphorus (P) Concentration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Boxplot Potássio (K) por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de concentração de potássio (K) ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='K', palette='Set2')\n",
    "plt.title('Potassium Levels by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Potassium (K) Concentration')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4. Boxplot Temperatura por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de temperatura ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='temperature', palette='Set2')\n",
    "plt.title('Temperature by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Temperature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5. Boxplot Umidade (%) por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de umidade (%) ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='humidity', palette='Set2')\n",
    "plt.title('Humidity by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6. Boxplot pH por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de pH ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='ph', palette='Set2')\n",
    "plt.title('pH by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('pH')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.7. Boxplot quantidade de precipitação por cultura agrícola\n",
    "\n",
    "Por meio desta visualização conseguimos avaliar o intervalo de quantidade de precipitação ideal para cada cultura agrícola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='label', y='rainfall', palette='Set2')\n",
    "plt.title('Rainfall by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Gráficos de barras para média de temperatura e umidade por cultura agrícola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Média de temperatura por cultura agrícola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='label', y='temperature', palette='Set2')\n",
    "plt.title('Temperature by Crop Type')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Temperature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Média de umidade por cultura agrícola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='label', y='humidity', palette='Set2', estimator=sum)\n",
    "plt.title('Average Humidity by Crop Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Média de precipitação por cultura agrícola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='label', y='rainfall', palette='Set2', estimator=sum)\n",
    "plt.title('Average Rainfall by Crop Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Crop Type')\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Gráficos de barras para média de quantidade de precipitação por cultura agrícola\n",
    "\n",
    "A linha ciano representa a estimativa da Função de Densidade de Probabilidade (PDF) da quantidade de precipitação para cada cultura agrícola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3. Rainfall distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['rainfall'], bins=20, kde=True, color='cyan')\n",
    "plt.title('Rainfall Distribution')\n",
    "plt.xlabel('Rainfall (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Matriz de correlação entre as variáveis numéricas\n",
    "\n",
    "Escala de cores baseada em calor:\n",
    "- Azul indica menor correlação (em valores absolutos)\n",
    "- Vermelho indica maior a correlação (em valores absolutos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação entre as features numéricas\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.select_dtypes(include=numerics).corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Matriz de Correlação\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Gráficos de pontos de temperatura por umidade para cada cultura agrícola\n",
    "\n",
    "Objetivos:\n",
    "- Avaliar os intervalos ideais de temperatura e umidade para cada cultura agrícola.\n",
    "- Responder à pergunta: `Encontrar o “perfil ideal” de solo/clima para as plantações`\n",
    "\n",
    "A fim de cumprir o objetivo, esta visualização mostra um gráfico para cada cultura e fixa os intevalos dos eixos x e y.\n",
    "- Eixo x: temperatura fixada entre 0 e 40 ºC\n",
    "- Eixo y: umidade fixada entre 0 e 100 %\n",
    "\n",
    "Desta forma, conseguimos visualizar a distribuição dos dados de temperatura e umidade para cada cultura agrícola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culturas = df['label'].unique()\n",
    "\n",
    "# Definir limites fixos com base nos dados, adicionando uma margem\n",
    "temp_min_fixed = 00  # Um pouco abaixo do mínimo\n",
    "temp_max_fixed = 45  # Um pouco acima do máximo\n",
    "hum_min_fixed = 0   # Um pouco abaixo do mínimo\n",
    "hum_max_fixed = 100   # Um pouco acima do máximo\n",
    "\n",
    "# Loop para criar um gráfico para cada cultura\n",
    "for cultura in culturas:\n",
    "    # Filtrar os dados para a cultura atual\n",
    "    data_cultura = df[df['label'] == cultura]\n",
    "    \n",
    "    # Criar o gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='temperature', y='humidity', data=data_cultura)\n",
    "    plt.title(f'Temperatura vs. Umidade para {cultura.capitalize()}')\n",
    "    plt.xlabel('Temperatura (°C)')\n",
    "    plt.ylabel('Umidade (%)')\n",
    "    \n",
    "    # Definir os limites dos eixos\n",
    "    plt.xlim(temp_min_fixed, temp_max_fixed)\n",
    "    plt.ylim(hum_min_fixed, hum_max_fixed)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Criar o gráfico de dispersão\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='temperature', y='humidity', hue='label',data=df)\n",
    "plt.title(f'Temperatura vs. Umidade')\n",
    "plt.xlabel('Temperatura (°C)')\n",
    "plt.ylabel('Umidade (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Distribuição de valores de pH (empilhada) por cultura agrícola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='ph', hue='label', multiple='stack', bins=20)\n",
    "plt.title('Distribuição do pH do Solo por Cultura')\n",
    "plt.xlabel('pH do Solo')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Gráficos de correlação bi-variadas para cada cultura agrícola\n",
    "\n",
    "Objetivo:\n",
    "- Avaliar a relação entre as variáveis para cada cultura agrícola\n",
    "\n",
    "Obs.: vamos fazer apenas para a cultura agrícola 'coffee' (café) para demonstração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlações bi-variadas para uma cultura específica\n",
    "_ = sns.pairplot(df[df[\"label\"] == \"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de dispersão 3D para N, P e K\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Converter labels para números\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "labels = le.classes_\n",
    "\n",
    "scatter = ax.scatter(df['N'], df['P'], df['K'], c=df['label_encoded'], cmap='viridis')\n",
    "ax.set_title('N vs. P vs. K por Cultura')\n",
    "ax.set_xlabel('Nitrogênio (N)')\n",
    "ax.set_ylabel('Fósforo (P)')\n",
    "ax.set_zlabel('Potássio (K)')\n",
    "\n",
    "# Legenda personalizada\n",
    "legend = ax.legend(handles=scatter.legend_elements()[0], labels=labels)\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Principais achados da base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perfil Ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning\n",
    "\n",
    "Sumário:\n",
    "1. Pré-processamento de Dados: limpeza, transformação e feature engineering\n",
    "2. Dividir dos Dados\n",
    "3. Algoritmos\n",
    "   1. Treinamento\n",
    "   2. Avaliação\n",
    "4. Concluir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definição do Problema\n",
    "\n",
    "Objetivos:\n",
    "- Desenvolver modelos preditivos de classificação de cultura agrícola baseando-se nos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Pré-processamento de Dados\n",
    "\n",
    "O pré-processamento de dados é essencial para garantir que o modelo de Machine Learning receba dados de alta qualidade. Ele abrange desde a limpeza e transformação dos dados até a criação de novas features, passando pela identificação e tratamento de outliers. Cada uma dessas etapas contribui para a melhoria da precisão e eficácia do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Limpeza de Dados\n",
    "\n",
    "Tratamento de dados ausentes, duplicados e inconsistências.\n",
    "\n",
    "- Não aplicável, pois não há valores ausentes, duplicatas ou inconsistências."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3.1 Criação de Novas Features\n",
    "\n",
    "- Não aplicável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3.2 Transformação de Variáveis Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar label-encoding para a variável categórica 'label'\n",
    "\n",
    "df_label = df['label']\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_label = le.fit_transform(df_label)\n",
    "\n",
    "print(df_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.drop(columns=['label', 'label_encoded'], errors='ignore')\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% dos dados para teste\n",
    "df_train, df_test, df_train_label, df_test_label = train_test_split(df_features, df_label, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"Quantidade de linhas no dataset de treino: \", df_train.shape[0])\n",
    "print(\"Quantidade de linhas no dataset de teste: \", df_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Transformação de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.1 Normalização\n",
    "\n",
    "Isso é útil para algoritmos sensíveis à escala, como redes neurais.\n",
    "\n",
    "A normalização dos dados pode melhorar o desempenho de alguns algoritmos de aprendizado de máquina categóricos, mas não de todos. Aqui está uma análise de como a normalização afeta cada um dos algoritmos:\n",
    "\n",
    "Algoritmos afetados pela normalização:\n",
    "\n",
    "- **KNN (k-Nearest Neighbors)**: KNN usa a distância euclidiana para calcular a similaridade entre os pontos de dados. Quando as variáveis ​​têm escalas diferentes, as variáveis ​​com valores maiores podem ter um impacto desproporcional na distância. A normalização garante que todas as variáveis ​​tenham o mesmo peso na medida da distância, o que pode melhorar a precisão do KNN.\n",
    "- **Regressão Logística**: A regressão logística usa a função sigmóide, que é sensível à escala das variáveis ​​de entrada. Se algumas variáveis ​​tiverem valores muito maiores do que outras, elas podem dominar a função sigmóide, levando a uma convergência lenta ou mesmo a um desempenho ruim. A normalização pode ajudar a resolver esse problema, garantindo que todas as variáveis ​​tenham uma escala semelhante.\n",
    "- **SVM (Máquina de Vetores de Suporte)**: SVM também usa a distância euclidiana para encontrar o hiperplano ótimo. Da mesma forma com o KNN, variáveis ​​com escalas diferentes podem influenciar a distância de forma desigual. A normalização pode ajudar a melhorar o desempenho do SVM, especialmente em conjuntos de dados com características de diferentes escalas.\n",
    "\n",
    "Algoritmos menos afetados pela normalização:\n",
    "\n",
    "- **Árvores de Decisão**: Árvores de decisão são menos sensíveis à escala dos dados, pois a divisão de nós é baseada na seleção da melhor variável e ponto de corte, independentemente de sua escala.\n",
    "- **Florestas Aleatórias**: Como as árvores de decisão, as florestas aleatórias são relativamente insensíveis à normalização, pois são compostas por várias árvores de decisão, que são treinadas em diferentes subconjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(df_train)\n",
    "df_train_scaled = scaler.transform(df_train)\n",
    "df_test_scaled = scaler.transform(df_test)\n",
    "\n",
    "# Verificar os dados normalizados\n",
    "df_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.2 Padronização\n",
    "\n",
    "Importante para algoritmos sensíveis à escala.\n",
    "\n",
    "Aplicável?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Árvore de Decisão\n",
    "\n",
    "Uma técnica de aprendizado supervisionado que cria uma estrutura hierárquica para classificação e regressão, baseada em decisões sequenciais.\n",
    "\n",
    "**Funcionamento:**\n",
    "- **Seleção da melhor divisão**: Identifica a variável que melhor divide os dados em subconjuntos homogêneos\n",
    "- **Divisão recursiva**: Divide os dados em subconjuntos, formando novos nós\n",
    "- **Classificação**: Classifica novos exemplos percorrendo a árvore da raiz até as folhas\n",
    "\n",
    "**Vantagens:**\n",
    "- Fácil interpretação\n",
    "- Flexibilidade para diferentes tipos de dados\n",
    "- Lida bem com variáveis numéricas e categóricas\n",
    "\n",
    "**Desvantagens:**\n",
    "- Tendência a overfitting\n",
    "- Instabilidade com pequenas variações nos dados\n",
    "- Viés para variáveis com mais categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(df_train_scaled, df_train_label)\n",
    "\n",
    "# Prever as classes para o conjunto de teste\n",
    "dt_pred = dt.predict(df_test_scaled)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(df_test_label, dt_pred)\n",
    "print(f'Acurácia do modelo Decision Tree: {accuracy:.2f}')\n",
    "\n",
    "# Gerar um relatório de classificação\n",
    "print(classification_report(df_test_label, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2. Floresta Aleatória\n",
    "\n",
    "Uma evolução da árvore de decisão que combina múltiplas árvores para melhorar a capacidade preditiva.\n",
    "\n",
    "**Funcionamento:**\n",
    "- **Criação de múltiplas árvores**: Treina várias árvores com subconjuntos aleatórios dos dados\n",
    "- **Agregação**: Combina as previsões de todas as árvores por votação ou média\n",
    "\n",
    "**Vantagens:**\n",
    "- Alta precisão\n",
    "- Robustez contra overfitting\n",
    "- Boa performance com dados de alta dimensionalidade\n",
    "\n",
    "**Desvantagens:**\n",
    "- Menor interpretabilidade\n",
    "- Alto custo computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=25)\n",
    "rf.fit(df_train_scaled, df_train_label)\n",
    "\n",
    "rf_pred = rf.predict(df_test_scaled)\n",
    "\n",
    "print(\"Acurácia do modelo Random Forest: \", accuracy_score(df_test_label, rf_pred))\n",
    "print(classification_report(df_test_label, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. Regressão Logística\n",
    "\n",
    "Algoritmo de aprendizado supervisionado que pode ser adaptado para classificação multiclasse através da abordagem \"one-vs-all\" ou \"one-vs-one\".\n",
    "\n",
    "**Funcionamento:**\n",
    "- **Combinação linear**: Combina características com seus pesos para cada classe\n",
    "- **Função logística**: Aplica função softmax para obter probabilidades para todas as classes\n",
    "- **Classificação**: Atribui o exemplo à classe com maior probabilidade prevista\n",
    "\n",
    "**Vantagens:**\n",
    "- Alta interpretabilidade das probabilidades por classe\n",
    "- Eficiente para grandes datasets\n",
    "- Robusta com regularização\n",
    "- Fornece probabilidades bem calibradas\n",
    "- Fácil de atualizar com novos dados\n",
    "\n",
    "**Desvantagens:**\n",
    "- Performance pode ser inferior a outros métodos em relações não-lineares complexas\n",
    "- Sensível à multicolinearidade entre variáveis preditoras\n",
    "- Pode requerer mais recursos computacionais conforme aumenta o número de classes\n",
    "- Necessita de uma quantidade significativa de dados por classe\n",
    "- Assume independência entre as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(df_train_scaled, df_train_label)\n",
    "\n",
    "lr_pred = lr.predict(df_test_scaled)\n",
    "\n",
    "print(\"Acurácia do modelo Logistic Regression: \", accuracy_score(df_test_label, lr_pred))\n",
    "print(classification_report(df_test_label, lr_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 K-Nearest Neighbors (KNN)\n",
    "\n",
    "Algoritmo baseado na proximidade entre pontos para classificação.\n",
    "\n",
    "**Funcionamento:**\n",
    "- **Cálculo de distância**: Mede a distância entre o novo ponto e todos os pontos existentes\n",
    "- **Seleção de vizinhos**: Identifica os K vizinhos mais próximos\n",
    "- **Votação**: Classifica baseado na maioria dos vizinhos\n",
    "\n",
    "**Vantagens:**\n",
    "- Simples de implementar\n",
    "- Não requer treinamento\n",
    "- Adaptável a novos dados\n",
    "\n",
    "**Desvantagens:**\n",
    "- Computacionalmente intensivo para grandes datasets\n",
    "- Sensível à escolha do valor K\n",
    "- Requer normalização dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(df_train_scaled, df_train_label)\n",
    "\n",
    "knn_pred = knn.predict(df_test_scaled)\n",
    "\n",
    "print(\"Acurácia do modelo KNN: \", accuracy_score(df_test_label, knn_pred))\n",
    "print(classification_report(df_test_label, knn_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5 Support Vector Machine (SVM)\n",
    "\n",
    "Algoritmo que busca o melhor hiperplano de separação entre classes.\n",
    "\n",
    "**Funcionamento:**\n",
    "- **Mapeamento dimensional**: Transforma dados para dimensões superiores\n",
    "- **Otimização**: Encontra o hiperplano com maior margem entre classes\n",
    "- **Classificação**: Determina classes pela posição relativa ao hiperplano\n",
    "\n",
    "**Vantagens:**\n",
    "- Eficaz em alta dimensionalidade\n",
    "- Robusto a outliers\n",
    "- Versátil com diferentes kernels\n",
    "\n",
    "**Desvantagens:**\n",
    "- Complexidade na escolha de parâmetros\n",
    "- Alto custo computacional para grandes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(df_train_scaled, df_train_label)\n",
    "\n",
    "svm_pred = svm.predict(df_test_scaled)\n",
    "\n",
    "print(\"Acurácia do modelo SVM: \", accuracy_score(df_test_label, svm_pred))\n",
    "print(classification_report(df_test_label, svm_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "[[0.59285714 0.39285714 0.155      ... 0.76867044 0.55544287 0.64871193]\n",
      " [0.56428571 0.32857143 0.055      ... 0.63276254 0.50016844 0.27292564]\n",
      " [0.25714286 0.44285714 0.36       ... 0.06173618 0.5987286  0.21099654]\n",
      " ...\n",
      " [0.07857143 0.22142857 0.13       ... 0.43766892 0.48089445 0.28658863]\n",
      " [0.07857143 0.85       0.995      ... 0.76775145 0.46100935 0.18268542]\n",
      " [0.22857143 0.52142857 0.085      ... 0.56108125 0.57336975 0.11790781]]\n",
      "\n",
      "\n",
      "[[0.72142857 0.08571429 0.21       ... 0.93886226 0.43037703 0.02033167]\n",
      " [0.7        0.02142857 0.23       ... 0.84309053 0.44329928 0.10358311]\n",
      " [0.42142857 0.40714286 0.22       ... 0.92278332 0.56196826 0.33886585]\n",
      " ...\n",
      " [0.25       0.96428571 0.96       ... 0.79909581 0.41665804 0.16598791]\n",
      " [0.06428571 0.02142857 0.05       ... 0.93451588 0.74514311 0.32246292]\n",
      " [0.5        0.27857143 0.07       ... 0.69059263 0.37254456 0.26512457]]\n",
      "Cross-validated accuracy for Decision Tree: 0.51 ± 0.05\n",
      "Test set accuracy for Decision Tree: 0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        62\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       0.00      0.00      0.00        56\n",
      "           3       0.00      0.00      0.00        68\n",
      "           4       0.00      0.00      0.00        61\n",
      "           5       0.87      1.00      0.93        60\n",
      "           6       0.30      1.00      0.47        56\n",
      "           7       1.00      1.00      1.00        59\n",
      "           8       0.13      0.83      0.23        54\n",
      "           9       0.00      0.00      0.00        71\n",
      "          10       0.15      1.00      0.26        55\n",
      "          11       0.98      0.87      0.92        60\n",
      "          12       0.77      0.96      0.85        55\n",
      "          13       0.00      0.00      0.00        70\n",
      "          14       0.00      0.00      0.00        58\n",
      "          15       1.00      1.00      1.00        57\n",
      "          16       0.00      0.00      0.00        60\n",
      "          17       0.00      0.00      0.00        61\n",
      "          18       0.00      0.00      0.00        60\n",
      "          19       0.00      0.00      0.00        60\n",
      "          20       0.00      0.00      0.00        58\n",
      "          21       0.00      0.00      0.00        62\n",
      "\n",
      "    accuracy                           0.42      1320\n",
      "   macro avg       0.33      0.44      0.35      1320\n",
      "weighted avg       0.32      0.42      0.34      1320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy for Random Forest: 0.99 ± 0.01\n",
      "Test set accuracy for Random Forest: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        62\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       1.00      1.00      1.00        56\n",
      "           3       1.00      1.00      1.00        68\n",
      "           4       1.00      1.00      1.00        61\n",
      "           5       1.00      1.00      1.00        60\n",
      "           6       0.95      1.00      0.97        56\n",
      "           7       1.00      1.00      1.00        59\n",
      "           8       0.84      0.98      0.91        54\n",
      "           9       1.00      1.00      1.00        71\n",
      "          10       0.98      0.96      0.97        55\n",
      "          11       1.00      0.95      0.97        60\n",
      "          12       1.00      1.00      1.00        55\n",
      "          13       0.97      0.99      0.98        70\n",
      "          14       1.00      1.00      1.00        58\n",
      "          15       1.00      1.00      1.00        57\n",
      "          16       1.00      1.00      1.00        60\n",
      "          17       1.00      1.00      1.00        61\n",
      "          18       1.00      1.00      1.00        60\n",
      "          19       1.00      1.00      1.00        60\n",
      "          20       0.98      0.83      0.90        58\n",
      "          21       1.00      1.00      1.00        62\n",
      "\n",
      "    accuracy                           0.99      1320\n",
      "   macro avg       0.99      0.99      0.99      1320\n",
      "weighted avg       0.99      0.99      0.99      1320\n",
      "\n",
      "Cross-validated accuracy for Logistic Regression: 0.87 ± 0.02\n",
      "Test set accuracy for Logistic Regression: 0.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        62\n",
      "           1       0.98      1.00      0.99        57\n",
      "           2       0.75      0.88      0.81        56\n",
      "           3       1.00      1.00      1.00        68\n",
      "           4       0.95      1.00      0.98        61\n",
      "           5       0.98      1.00      0.99        60\n",
      "           6       0.90      1.00      0.95        56\n",
      "           7       1.00      1.00      1.00        59\n",
      "           8       0.89      0.74      0.81        54\n",
      "           9       0.97      0.94      0.96        71\n",
      "          10       0.66      0.93      0.77        55\n",
      "          11       1.00      0.92      0.96        60\n",
      "          12       0.64      1.00      0.78        55\n",
      "          13       1.00      0.23      0.37        70\n",
      "          14       0.88      1.00      0.94        58\n",
      "          15       0.50      1.00      0.66        57\n",
      "          16       1.00      0.92      0.96        60\n",
      "          17       1.00      0.74      0.85        61\n",
      "          18       0.93      0.83      0.88        60\n",
      "          19       0.97      1.00      0.98        60\n",
      "          20       0.75      0.93      0.83        58\n",
      "          21       1.00      0.06      0.12        62\n",
      "\n",
      "    accuracy                           0.86      1320\n",
      "   macro avg       0.90      0.87      0.84      1320\n",
      "weighted avg       0.90      0.86      0.84      1320\n",
      "\n",
      "Cross-validated accuracy for KNN: 0.97 ± 0.01\n",
      "Test set accuracy for KNN: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        62\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       0.83      0.96      0.89        56\n",
      "           3       1.00      1.00      1.00        68\n",
      "           4       1.00      1.00      1.00        61\n",
      "           5       0.98      0.98      0.98        60\n",
      "           6       0.92      1.00      0.96        56\n",
      "           7       1.00      0.98      0.99        59\n",
      "           8       0.74      0.98      0.84        54\n",
      "           9       0.97      1.00      0.99        71\n",
      "          10       0.81      0.93      0.86        55\n",
      "          11       1.00      0.92      0.96        60\n",
      "          12       0.92      1.00      0.96        55\n",
      "          13       0.98      0.80      0.88        70\n",
      "          14       0.95      1.00      0.97        58\n",
      "          15       1.00      1.00      1.00        57\n",
      "          16       1.00      0.98      0.99        60\n",
      "          17       1.00      0.92      0.96        61\n",
      "          18       1.00      0.82      0.90        60\n",
      "          19       0.98      1.00      0.99        60\n",
      "          20       0.98      0.71      0.82        58\n",
      "          21       1.00      1.00      1.00        62\n",
      "\n",
      "    accuracy                           0.95      1320\n",
      "   macro avg       0.96      0.95      0.95      1320\n",
      "weighted avg       0.96      0.95      0.95      1320\n",
      "\n",
      "Cross-validated accuracy for SVM: 0.98 ± 0.01\n",
      "Test set accuracy for SVM: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        62\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       0.90      0.95      0.92        56\n",
      "           3       1.00      1.00      1.00        68\n",
      "           4       1.00      1.00      1.00        61\n",
      "           5       1.00      1.00      1.00        60\n",
      "           6       0.95      1.00      0.97        56\n",
      "           7       1.00      1.00      1.00        59\n",
      "           8       0.75      1.00      0.86        54\n",
      "           9       0.97      1.00      0.99        71\n",
      "          10       0.88      0.95      0.91        55\n",
      "          11       1.00      0.95      0.97        60\n",
      "          12       0.96      1.00      0.98        55\n",
      "          13       1.00      0.91      0.96        70\n",
      "          14       1.00      1.00      1.00        58\n",
      "          15       1.00      1.00      1.00        57\n",
      "          16       1.00      1.00      1.00        60\n",
      "          17       1.00      0.98      0.99        61\n",
      "          18       1.00      0.92      0.96        60\n",
      "          19       1.00      1.00      1.00        60\n",
      "          20       0.98      0.69      0.81        58\n",
      "          21       1.00      1.00      1.00        62\n",
      "\n",
      "    accuracy                           0.97      1320\n",
      "   macro avg       0.97      0.97      0.97      1320\n",
      "weighted avg       0.97      0.97      0.97      1320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Split the data\n",
    "df_train, df_test = train_test_split(df, test_size=0.6, random_state=42)\n",
    "\n",
    "# Then perform label encoding on the training set\n",
    "le = LabelEncoder()\n",
    "df_train['label_encoded'] = le.fit_transform(df_train['label'])\n",
    "\n",
    "# Apply the same encoder to the test set without fitting\n",
    "df_test['label_encoded'] = le.transform(df_test['label'])\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_train_scaled = scaler.fit_transform(df_train.drop(columns=['label', 'label_encoded']))\n",
    "df_test_scaled = scaler.transform(df_test.drop(columns=['label', 'label_encoded']))\n",
    "\n",
    "print(df_train_scaled)\n",
    "print(\"\\n\")\n",
    "print(df_test_scaled)\n",
    "\n",
    "# Function to perform cross-validation and display results\n",
    "def evaluate_model(model, model_name):\n",
    "    model.fit(df_train_scaled, df_train['label_encoded'])\n",
    "\n",
    "    # Cross-validation accuracy\n",
    "    cv_scores = cross_val_score(model, df_train_scaled, df_train['label_encoded'], cv=5)\n",
    "    print(f'Cross-validated accuracy for {model_name}: {np.mean(cv_scores):.2f} ± {np.std(cv_scores):.2f}')\n",
    "\n",
    "    # Test set accuracy\n",
    "    predictions = model.predict(df_test_scaled)\n",
    "    print(f'Test set accuracy for {model_name}: {accuracy_score(df_test['label_encoded'], predictions):.2f}')\n",
    "    print(classification_report(df_test['label_encoded'], predictions))\n",
    "\n",
    "# Decision Tree Classifier with max depth to reduce overfitting\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "evaluate_model(dt, \"Decision Tree\")\n",
    "\n",
    "# Random Forest Classifier with limited depth\n",
    "rf = RandomForestClassifier(n_estimators=25, max_depth=10)\n",
    "evaluate_model(rf, \"Random Forest\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "evaluate_model(lr, \"Logistic Regression\")\n",
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "evaluate_model(knn, \"KNN\")\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC()\n",
    "evaluate_model(svm, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "Accuracy: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        48\n",
      "      banana       1.00      1.00      1.00        36\n",
      "   blackgram       0.70      0.95      0.80        37\n",
      "    chickpea       1.00      1.00      1.00        47\n",
      "     coconut       0.00      0.00      0.00        43\n",
      "      coffee       1.00      1.00      1.00        39\n",
      "      cotton       1.00      1.00      1.00        40\n",
      "      grapes       1.00      1.00      1.00        36\n",
      "        jute       1.00      0.21      0.35        43\n",
      " kidneybeans       1.00      1.00      1.00        45\n",
      "      lentil       0.52      1.00      0.69        37\n",
      "       maize       1.00      0.89      0.94        37\n",
      "       mango       1.00      1.00      1.00        41\n",
      "   mothbeans       0.00      0.00      0.00        47\n",
      "    mungbean       1.00      1.00      1.00        37\n",
      "   muskmelon       1.00      1.00      1.00        35\n",
      "      orange       0.85      1.00      0.92        35\n",
      "      papaya       1.00      0.12      0.22        40\n",
      "  pigeonpeas       1.00      1.00      1.00        46\n",
      " pomegranate       0.52      1.00      0.68        44\n",
      "        rice       0.32      1.00      0.49        33\n",
      "  watermelon       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.81       880\n",
      "   macro avg       0.81      0.83      0.78       880\n",
      "weighted avg       0.81      0.81      0.77       880\n",
      "\n",
      "Predicted label: rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Feature columns\n",
    "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
    "# Label column\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=7)\n",
    "clf.fit(X_train, y_train) # Train the model\n",
    "y_pred = clf.predict(X_test) # Make predictions\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making a prediction for a new sample\n",
    "new_sample = [[85, 40, 42, 21.0, 80.0, 6.4, 200.0]]  # Example input\n",
    "predicted_label = clf.predict(new_sample)\n",
    "print(f'Predicted label: {predicted_label[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "Accuracy: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        48\n",
      "      banana       1.00      1.00      1.00        36\n",
      "   blackgram       1.00      1.00      1.00        37\n",
      "    chickpea       1.00      1.00      1.00        47\n",
      "     coconut       1.00      1.00      1.00        43\n",
      "      coffee       1.00      1.00      1.00        39\n",
      "      cotton       1.00      1.00      1.00        40\n",
      "      grapes       1.00      1.00      1.00        36\n",
      "        jute       0.89      0.98      0.93        43\n",
      " kidneybeans       1.00      1.00      1.00        45\n",
      "      lentil       0.97      1.00      0.99        37\n",
      "       maize       1.00      1.00      1.00        37\n",
      "       mango       1.00      1.00      1.00        41\n",
      "   mothbeans       1.00      0.98      0.99        47\n",
      "    mungbean       1.00      1.00      1.00        37\n",
      "   muskmelon       1.00      1.00      1.00        35\n",
      "      orange       1.00      1.00      1.00        35\n",
      "      papaya       1.00      1.00      1.00        40\n",
      "  pigeonpeas       1.00      1.00      1.00        46\n",
      " pomegranate       1.00      1.00      1.00        44\n",
      "        rice       0.97      0.85      0.90        33\n",
      "  watermelon       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.99       880\n",
      "   macro avg       0.99      0.99      0.99       880\n",
      "weighted avg       0.99      0.99      0.99       880\n",
      "\n",
      "Predicted label: rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Feature columns (adjust these according to your dataset's actual structure)\n",
    "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
    "# Label column\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Feature scaling (optional but recommended for consistent results)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of trees\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making a prediction for a new sample\n",
    "new_sample = [[85, 40, 42, 21.0, 80.0, 6.4, 200.0]]  # Example input\n",
    "new_sample_scaled = scaler.transform(new_sample)  # Scale the new sample\n",
    "predicted_label = rf.predict(new_sample_scaled)\n",
    "print(f'Predicted label: {predicted_label[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        48\n",
      "      banana       0.97      1.00      0.99        36\n",
      "   blackgram       0.74      0.92      0.82        37\n",
      "    chickpea       1.00      1.00      1.00        47\n",
      "     coconut       1.00      1.00      1.00        43\n",
      "      coffee       0.97      1.00      0.99        39\n",
      "      cotton       0.90      0.93      0.91        40\n",
      "      grapes       1.00      1.00      1.00        36\n",
      "        jute       0.88      0.86      0.87        43\n",
      " kidneybeans       0.98      0.98      0.98        45\n",
      "      lentil       0.90      0.95      0.92        37\n",
      "       maize       0.91      0.86      0.89        37\n",
      "       mango       1.00      1.00      1.00        41\n",
      "   mothbeans       0.94      0.70      0.80        47\n",
      "    mungbean       0.93      1.00      0.96        37\n",
      "   muskmelon       1.00      1.00      1.00        35\n",
      "      orange       1.00      1.00      1.00        35\n",
      "      papaya       0.95      0.97      0.96        40\n",
      "  pigeonpeas       0.98      0.93      0.96        46\n",
      " pomegranate       1.00      1.00      1.00        44\n",
      "        rice       0.88      0.85      0.86        33\n",
      "  watermelon       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.95       880\n",
      "   macro avg       0.95      0.95      0.95       880\n",
      "weighted avg       0.95      0.95      0.95       880\n",
      "\n",
      "Predicted label: rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Feature columns\n",
    "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
    "# Label column\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making a prediction for a new sample\n",
    "new_sample = [[85, 40, 42, 21.0, 80.0, 6.4, 200.0]]  # Example input\n",
    "predicted_label = model.predict(new_sample)\n",
    "print(f'Predicted label: {predicted_label[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "Accuracy: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        48\n",
      "      banana       1.00      1.00      1.00        36\n",
      "   blackgram       0.90      0.97      0.94        37\n",
      "    chickpea       1.00      1.00      1.00        47\n",
      "     coconut       0.93      1.00      0.97        43\n",
      "      coffee       0.97      0.95      0.96        39\n",
      "      cotton       0.95      1.00      0.98        40\n",
      "      grapes       1.00      1.00      1.00        36\n",
      "        jute       0.85      0.95      0.90        43\n",
      " kidneybeans       0.96      1.00      0.98        45\n",
      "      lentil       0.90      0.95      0.92        37\n",
      "       maize       0.97      0.95      0.96        37\n",
      "       mango       0.95      1.00      0.98        41\n",
      "   mothbeans       1.00      0.94      0.97        47\n",
      "    mungbean       1.00      1.00      1.00        37\n",
      "   muskmelon       1.00      1.00      1.00        35\n",
      "      orange       1.00      0.91      0.96        35\n",
      "      papaya       1.00      1.00      1.00        40\n",
      "  pigeonpeas       1.00      0.87      0.93        46\n",
      " pomegranate       1.00      1.00      1.00        44\n",
      "        rice       0.96      0.82      0.89        33\n",
      "  watermelon       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.97       880\n",
      "   macro avg       0.97      0.97      0.97       880\n",
      "weighted avg       0.97      0.97      0.97       880\n",
      "\n",
      "Predicted label: rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Feature columns\n",
    "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
    "# Label column\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a KNN Classifier\n",
    "k = 3  # You can experiment with different values of k\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making a prediction for a new sample\n",
    "new_sample = [[85, 40, 42, 21.0, 80.0, 6.4, 200.0]]  # Example input\n",
    "new_sample_scaled = scaler.transform(new_sample)  # Scale the new sample\n",
    "predicted_label = knn.predict(new_sample_scaled)\n",
    "print(f'Predicted label: {predicted_label[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N   P   K  temperature   humidity        ph    rainfall label\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice\n",
      "Accuracy: 0.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       1.00      1.00      1.00        48\n",
      "      banana       1.00      0.97      0.99        36\n",
      "   blackgram       0.83      0.92      0.87        37\n",
      "    chickpea       1.00      1.00      1.00        47\n",
      "     coconut       1.00      0.98      0.99        43\n",
      "      coffee       1.00      0.85      0.92        39\n",
      "      cotton       1.00      0.88      0.93        40\n",
      "      grapes       1.00      1.00      1.00        36\n",
      "        jute       0.73      0.86      0.79        43\n",
      " kidneybeans       0.98      1.00      0.99        45\n",
      "      lentil       0.85      0.92      0.88        37\n",
      "       maize       0.46      1.00      0.63        37\n",
      "       mango       0.97      0.95      0.96        41\n",
      "   mothbeans       1.00      0.77      0.87        47\n",
      "    mungbean       0.88      1.00      0.94        37\n",
      "   muskmelon       1.00      1.00      1.00        35\n",
      "      orange       1.00      0.91      0.96        35\n",
      "      papaya       1.00      0.70      0.82        40\n",
      "  pigeonpeas       1.00      0.72      0.84        46\n",
      " pomegranate       0.95      0.89      0.92        44\n",
      "        rice       0.92      0.70      0.79        33\n",
      "  watermelon       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.91       880\n",
      "   macro avg       0.94      0.91      0.91       880\n",
      "weighted avg       0.94      0.91      0.91       880\n",
      "\n",
      "Predicted label: rice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisfuturist/GitHub/FIAP/activity-fiap-ai-p3a4/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Feature columns (assuming these are the correct feature names; adjust if necessary)\n",
    "X = df[['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']]\n",
    "# Label column\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM Classifier\n",
    "svm = SVC(kernel='poly')  # You can choose different kernels like 'linear', 'poly', 'sigmoid'\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making a prediction for a new sample\n",
    "new_sample = [[85, 40, 42, 21.0, 80.0, 6.4, 200.0]]  # Example input\n",
    "new_sample_scaled = scaler.transform(new_sample)  # Scale the new sample\n",
    "predicted_label = svm.predict(new_sample_scaled)\n",
    "print(f'Predicted label: {predicted_label[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
